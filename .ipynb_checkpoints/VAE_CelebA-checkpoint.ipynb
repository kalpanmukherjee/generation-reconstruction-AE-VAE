{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for pulling up images and annotations\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" #for apple sillicon macs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 512  # Size of the latent space\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc4771",
   "metadata": {},
   "source": [
    "## Count number of images available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_files_in_directory(directory):\n",
    "    # List the files in the directory\n",
    "    file_list = os.listdir(directory)\n",
    "\n",
    "    file_count = sum(os.path.isfile(os.path.join(directory, f)) for f in file_list)\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb2493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = 'CelebA/Img/img_align_celeba/'\n",
    "\n",
    "number_of_images_in_celebA = count_num_files_in_directory(directory)\n",
    "print(\"num images: \", number_of_images_in_celebA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8360056",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, directory, split='train', split_ratio = 0.8, num_samples=None, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.image_filenames = self.image_filenames[:int(split_ratio * len(self.image_filenames))]\n",
    "        elif split == 'test':\n",
    "            self.image_filenames = self.image_filenames[int(split_ratio * len(self.image_filenames)):]\n",
    "        \n",
    "    def __len__(self):\n",
    "#         return 20000 # if we want to use only a subset of the dataset\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.directory, self.image_filenames[idx])\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Define a transform to preprocess the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "celeba_train_dataset = CelebADataset(directory=directory, split='train', split_ratio = 0.8, transform=transform)\n",
    "celeba_test_dataset = CelebADataset(directory=directory, split='test', split_ratio = 0.8, transform=transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(celeba_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(celeba_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57eabd",
   "metadata": {},
   "source": [
    "## Display images using dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98172f8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Now you can iterate over the dataloader\n",
    "for i, images in enumerate(train_dataloader):\n",
    "    # Choose just one image to display from the batch\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    # Convert the tensor to a numpy array and transpose the dimensions from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(images[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "    plt.show()\n",
    "    \n",
    "    # Break after the first image to prevent displaying all images\n",
    "    if i == 0:  # display just one batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45183043",
   "metadata": {},
   "source": [
    "## Basic Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), stride=(2,2), padding=1) # input image size is 3 x 64 x 64\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3,3), stride=(2,2), padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3,3), stride=(2,2), padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3,3), stride=(2,2), padding=1)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=(3,3), stride=(2,2), padding=1)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(512)\n",
    "        self.fc_mu = nn.Linear(2048, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(2048, latent_dim)\n",
    "        self.lin = nn.Linear(2048,latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 2048)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=(3,3), stride=(2,2), padding=1, output_padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(256)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=(3,3), stride=(2,2), padding=1, output_padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=(3,3), stride=(2,2), padding=1, output_padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=(3,3), stride=(2,2), padding=1, output_padding=1)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 32, kernel_size=(3,3), stride=(2,2), padding=1, output_padding=1)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(32)\n",
    "        self.conv = nn.Conv2d(32, 3, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 512, 2, 2) # reshaping to match the corresponding encoder shape\n",
    "        x = self.deconv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = self.deconv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = self.deconv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "        x = self.deconv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "        x = self.deconv5(x)\n",
    "        x = F.relu(self.batchnorm5(x))\n",
    "        x = self.conv(x)\n",
    "        reconstruction = torch.tanh(x) # Using sigmoid for final layer to output values between 0 and 1\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3576d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variational autoencoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std, std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z, std = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "def vae_loss(recon_x, x, mu, std):\n",
    "    mu = mu.to(device)\n",
    "    std = std.to(device)\n",
    "    BCE = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + 2*std - (2*std).exp() - mu.pow(2))\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07381920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(recon_x, x):\n",
    "#     loss = F.mse_loss(recon_x, x, reduction='mean')\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(epoch, loss, filename):\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch, loss])\n",
    "\n",
    "filename = '200k_training_log.csv'\n",
    "\n",
    "# Initialize or clear the file with headers\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ee204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, dataloader, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, std = model(data)\n",
    "            loss = vae_loss(recon_batch, data, mu, std)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print('Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(dataloader.dataset)))\n",
    "        save_to_csv(epoch,  train_loss / len(dataloader.dataset), filename)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad651758",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b718ca9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_loader = train_dataloader\n",
    "model = VAE(latent_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the VAE\n",
    "vae_model = train_vae(model, train_loader, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa74257",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = []\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    data = data.to(device)\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    temp = mu + eps * std\n",
    "    temp = temp.cpu()\n",
    "    for i in range(len(temp)):\n",
    "        val = temp[i][0].detach().numpy()\n",
    "        vector.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134572b1",
   "metadata": {},
   "source": [
    "# Plotting first dimension of the latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vector = np.array(vector)\n",
    "counts, bins = np.histogram(vector)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c500b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = img.to(device)\n",
    "reconstruction, mu, logvar = model(test_examples)\n",
    "reconstruction = reconstruction.to('cpu')\n",
    "reconstruction = reconstruction.to('cpu')\n",
    "print(img.shape)\n",
    "print(reconstruction.shape)\n",
    "with torch.no_grad():\n",
    "    number = 1\n",
    "    plt.figure(figsize=(40, 10))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, number, index+1)\n",
    "        test_examples = test_examples.to('cpu')\n",
    "        image1 = test_examples[index].permute(1, 2, 0).detach().numpy().clip(0, 1)\n",
    "        plt.imshow(image1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, number, 2*(index + 1))\n",
    "        image2 = reconstruction[index].permute(1, 2, 0).detach().numpy().clip(0, 1)\n",
    "        plt.imshow(image2)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(celeba_test_dataset, batch_size=36, shuffle=True)\n",
    "img = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = img.to(device)\n",
    "reconstruction, mu, logvar = model(test_examples)\n",
    "reconstruction = reconstruction.to('cpu')\n",
    "reconstruction = reconstruction.to('cpu')\n",
    "print(img.shape)\n",
    "print(reconstruction.shape)\n",
    "vutils.save_image(reconstruction.data,\n",
    "                  os.path.join(\n",
    "                      \"./\",\n",
    "                      f\"recons1.png\"),\n",
    "                  normalize=True,\n",
    "                  nrow=6)\n",
    "vutils.save_image(test_examples.data,\n",
    "                  os.path.join(\n",
    "                      \"./\",\n",
    "                      f\"img1.png\"),\n",
    "                  normalize=True,\n",
    "                  nrow=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
